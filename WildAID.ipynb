{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WildAID.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PM6fb0vDbhAa"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jtdsouza/Course-Overview/blob/master/WildAID.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jA2Q3Tn6YPbN",
        "colab_type": "text"
      },
      "source": [
        "# Initialization and Load Image Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVEyGQQmufJu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Set up Tensor flow 2.0\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u-mWgHlgum4P",
        "outputId": "a070d034-3ce2-46aa-bd11-449db66e0d8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import MaxPooling2D, AveragePooling2D, BatchNormalization, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
        "from tensorflow.keras.layers import Concatenate\n",
        "from tensorflow.keras.layers import Lambda, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.initializers import he_normal\n",
        "from tensorflow.keras.initializers import lecun_normal\n",
        "from tensorflow.keras.initializers import he_uniform\n",
        "from tensorflow.keras.initializers import glorot_uniform\n",
        "from tensorflow.keras.initializers import glorot_normal\n",
        "from tensorflow.keras import backend as K\n",
        "from keras.preprocessing import image as KImage\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import cv2\n",
        "import csv\n",
        "import os\n",
        "import numpy as np\n",
        "from numpy import genfromtxt\n",
        "import pandas as pd\n",
        "from google.colab.patches import cv2_imshow\n",
        "from collections import defaultdict\n",
        "import random\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input as VGG16Pre\n",
        "from tensorflow.keras.applications.vgg19 import VGG19\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input as VGG19Pre\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input as InceptionPre\n",
        "from tensorflow.keras.applications.xception import Xception\n",
        "from tensorflow.keras.applications.xception import preprocess_input as XceptionPre\n",
        "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as MNPre\n",
        "from tensorflow.keras.applications.resnet_v2 import ResNet152V2\n",
        "from tensorflow.keras.applications.resnet_v2 import ResNet50V2\n",
        "from tensorflow.keras.applications.nasnet import NASNetMobile\n",
        "from tensorflow.keras.applications.densenet import DenseNet121\n",
        "\n",
        "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import pickle\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import metrics\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOq_0bZOKqT8",
        "colab_type": "code",
        "outputId": "7c8d167a-1919-4ec7-9681-e5672e38c56b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#Mount Google Drive - Note this mounts your personal GDrive to the directory stated\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJpw6HR34887",
        "colab_type": "text"
      },
      "source": [
        "## Load Images and setup Data Structures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKqL-xdT9jcp",
        "colab_type": "text"
      },
      "source": [
        "Images are loaded from csv files that contain previously processed data sets.\n",
        "The preprocessing implementation can be found here: https://colab.research.google.com/drive/1tVg9y71wbf_-bpgOue4LAFbCSXuu2SCD?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39LcbZgrd4Pn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  #Set up path for csv files containing preprocessed images. CHange subfolder names to match your setup in google drive\n",
        "  csvpath='/content/drive/My Drive/WildAI/csv'\n",
        "\n",
        "  #Function to load processed image data in csv files (both training and test, input data labels)\n",
        "  def LoadData(train_imagefile=\"Training-Images-224.csv\",train_labelfile=\"Training-Labels-224.txt\",\n",
        "               test_imagefile=\"Test-Images-224.csv\",test_labelfile=\"Test-Labels-224.txt\"):\n",
        "    \n",
        "    #Training Data Set\n",
        "    X=[]\n",
        "    Individuals=[]\n",
        "    Species=[]\n",
        "    Ind_DB=defaultdict(defaultdict)\n",
        "\n",
        "    \n",
        "    dataset=np.loadtxt(os.path.join(csvpath,train_imagefile),delimiter=\",\")\n",
        "    f=open(os.path.join(csvpath,train_labelfile),'r')\n",
        "    lines=f.readlines()\n",
        "    for line in lines:\n",
        "      vals=line.rstrip()\n",
        "      Species.append(vals.split(\"-\")[0])\n",
        "      Individuals.append(vals)\n",
        "    i=0\n",
        "    for x in dataset:\n",
        "      image=x.reshape(224,224,3)\n",
        "      X.append(image)\n",
        "      species=Species[i]\n",
        "      key=Individuals[i]\n",
        "      spec_DB=Ind_DB[species]\n",
        "      if key not in spec_DB.keys():\n",
        "        spec_DB[key]=[image]\n",
        "      else:\n",
        "        spec_DB[key].append(image)\n",
        "      i=i+1\n",
        "\n",
        "\n",
        "    #Test Data Set\n",
        "    X_Test=[]\n",
        "    Individuals_Test=[]\n",
        "    Species_Test=[]\n",
        "    dataset=np.loadtxt(os.path.join(csvpath,test_imagefile),delimiter=\",\")\n",
        "\n",
        "    for x in dataset:\n",
        "      image=x.reshape(224,224,3)\n",
        "      X_Test.append(image)\n",
        "\n",
        "    f=open(os.path.join(csvpath,test_labelfile),'r')\n",
        "    lines=f.readlines()\n",
        "    for line in lines:\n",
        "      vals=line.rstrip()\n",
        "      Species_Test.append(vals.split(\"-\")[0])\n",
        "      Individuals_Test.append(vals)\n",
        "\n",
        "    X_Test=np.asarray(X_Test)\n",
        "    X=np.asarray(X)\n",
        "    return (X,Species,Individuals,Ind_DB,X_Test,Species_Test,Individuals_Test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ja697BTjQnXe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load Pre-Processed Images\n",
        "\n",
        "X,Species,Individuals, Ind_DB,X_Test,Species_Test,Individuals_Test=LoadData()\n",
        "\n",
        "#Use this line for augmented images\n",
        "#X,Species,Individuals,Ind_DB,X_Test,Species_Test,Individuals_Test=LoadData(train_imagefile=\"Training-Images-224.csv\",train_labelfile=\"Training-Labels-224.txt\",\n",
        "#              test_imagefile=\"Test-Images-224.csv\",test_labelfile=\"Test-Labels-224.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aan_gNm77C3w",
        "colab_type": "code",
        "outputId": "d3bd4f37-f6b6-427f-9f31-c726c85608d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "le = LabelEncoder()\n",
        "le.fit(Species)\n",
        "Y=le.transform(Species)\n",
        "Y_Test=le.transform(Species_Test)\n",
        "Y1=to_categorical(np.array(Y))\n",
        "Y_Test1=to_categorical(np.array(Y_Test))\n",
        "print(Y1.shape)\n",
        "print(le.classes_)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1437, 8)\n",
            "['Amur Tiger' 'Bengal Tiger' 'Black Rhino' 'Cheetah' 'Leopard'\n",
            " 'Lowland Tapir' 'Puma' 'White Rhino']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJN5V8YUugyG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#For Species Classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_Train, X_Val, Y_Train, Y_Val = train_test_split(X, Y1, test_size=0.10, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJaNHp6duki-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## DYNAMIC AUGMENTATION\n",
        "\n",
        "datagen = ImageDataGenerator(rescale=1/255.,rotation_range=30,\n",
        "\tzoom_range=0.15,\n",
        "\twidth_shift_range=0.2,\n",
        "\theight_shift_range=0.2,\n",
        "\tshear_range=0.15,\n",
        "\tfill_mode=\"nearest\",validation_split=0.0)\n",
        "training_generator = datagen.flow(X_Train, Y_Train, batch_size=32,shuffle=True,seed=7)\n",
        "validation_generator = datagen.flow(X_Val, Y_Val, batch_size=32,shuffle=True,seed=7)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-vnCtF1YyBQ",
        "colab_type": "text"
      },
      "source": [
        "# VGG16 Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffwUK27S-T7p",
        "colab_type": "text"
      },
      "source": [
        "Reference implementation for both Species Classification and Individual Identification done with VGG16 pretrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPPotfxT4UKa",
        "colab_type": "text"
      },
      "source": [
        "## Load/Setup Base Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjpNgt5baHX3",
        "colab_type": "code",
        "outputId": "ee3394be-ba8f-45e9-e4bf-55a72062f38a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "input_shape=(224,224,3)\n",
        "vgg=VGG16(weights='imagenet',include_top=False,input_shape=input_shape)\n",
        "vgg.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vr_J_LbTYaQm",
        "colab_type": "text"
      },
      "source": [
        "## Species Classification Task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyWuU_oDZHWn",
        "colab_type": "text"
      },
      "source": [
        "### PreTrained Network - Train all Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhGyQkKXO6Mz",
        "colab_type": "code",
        "outputId": "14ef9ddd-4223-40f2-900b-81c354e02aeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "#Set all layers of pretrained VGG16 model as trainable. Add a few dense layers on top\n",
        "\n",
        "vgg_model=Sequential()\n",
        "vgg_model.add(VGG16(weights='imagenet',include_top=False,input_shape=input_shape))\n",
        "vgg_model.add(Flatten())\n",
        "vgg_model.add(Dropout(0.4))\n",
        "vgg_model.add(Dense(256, activation='relu',name=\"Dense1\"))\n",
        "vgg_model.add(Dense(128, activation='relu'))\n",
        "vgg_model.add(Dense(64, activation='relu'))\n",
        "vgg_model.add(Dropout(0.4))\n",
        "vgg_model.add(Dense(8))\n",
        "\n",
        "vgg_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "vgg16 (Model)                (None, 7, 7, 512)         14714688  \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "Dense1 (Dense)               (None, 256)               6422784   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 8)                 520       \n",
            "=================================================================\n",
            "Total params: 21,179,144\n",
            "Trainable params: 21,179,144\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6nsK-Z95qtQ",
        "colab_type": "text"
      },
      "source": [
        "#### First Time training Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxMpxh8vfpxR",
        "colab_type": "code",
        "outputId": "4fc61479-a16d-4933-a1a8-038e8b1c4c81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "vgg_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.00001),\n",
        "              loss=loss_fn,\n",
        "              metrics=['accuracy'])\n",
        "#STATIC\n",
        "vgg_model.fit(X_Train,Y_Train,validation_data=(X_Val,Y_Val),epochs=30)\n",
        "\n",
        "#DYNAMIC ** Note: tried Validation without augmentation (from above) and got ~20% accuracy..\n",
        "#history = vgg_model.fit_generator(training_generator,steps_per_epoch=(len(X_Train))//32, validation_data=validation_generator,validation_steps=len(X_Val)//32,epochs=30)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-93be737105d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m               metrics=['accuracy'])\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#STATIC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mvgg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_Train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_Train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_Val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_Val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#DYNAMIC ** Note: tried Validation without augmentation (from above) and got ~20% accuracy..\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_Train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WE_Jsg91iO2y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Save model\n",
        "vgg_model.save_weights(os.path.join(csvpath,\"vgg-model.h5\"))\n",
        "#vgg_model.save_weights(os.path.join(csvpath,\"vgg-model-augmented.h5\"))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZOIKPCxnwkH",
        "colab_type": "code",
        "outputId": "80e26c34-3fdb-4045-cb4f-024ed46c32f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#EVal;uate on Test Data WITHOUT Augmentation\n",
        "vgg_model.evaluate(X_Test,  Y_Test1, verbose=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5/5 - 1s - loss: 0.0579 - accuracy: 0.9851\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.057863593101501465, 0.9850746393203735]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfouvJ0vIrUZ",
        "colab_type": "code",
        "outputId": "4e7caa96-0c82-49bf-d53e-9094c350b599",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#Evaluate on Test Data WITH Augmentation\n",
        "test_generator = datagen.flow(X_Test, Y_Test1, batch_size=32,shuffle=True,seed=7)\n",
        "vgg_model.evaluate(test_generator,steps=len(X_Test)//32)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 1s 262ms/step - loss: 0.1945 - accuracy: 0.9453\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.1945144236087799, 0.9453125]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ptUQW0jnyOy",
        "colab_type": "text"
      },
      "source": [
        "#### Subsequent Runs. - Reload model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmWZOXrWnesL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vgg_model.load_weights(os.path.join(csvpath,\"vgg-model.h5\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RW_NprOLqZGl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "vgg_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.00001),\n",
        "              loss=loss_fn,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRtD54SVndd0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_generator = datagen.flow(X_Test, Y_Test1, batch_size=32,shuffle=True,seed=7)\n",
        "vgg_model.evaluate(X_Test,  Y_Test1, verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xlcrIbVLR5i",
        "colab_type": "text"
      },
      "source": [
        "## Individual Identification Task\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S70fnzJ_6zyy",
        "colab_type": "text"
      },
      "source": [
        "### Common Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8Hu9t6gk01i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Functions used in TRIPLES Network Architecture (for Identification)\n",
        "\n",
        "# Assumes Ind_DB (see load data section) populated with training images, \n",
        "# creates set of triples for training a triplets network using batch size specified below.\n",
        "\n",
        "def GetTriples(batch_size=20,rnd=False):\n",
        "  if rnd:\n",
        "    sample_size=int(batch_size/len(Ind_DB.keys()))\n",
        "  triples=[np.zeros((batch_size,224,224,3))for i in range(3)]\n",
        "  cnt=0\n",
        "  while (cnt<batch_size):\n",
        "    for spec,inds in list(Ind_DB.items()):\n",
        "      names=list(inds.keys())\n",
        "      pop_size=len(names)\n",
        "      if pop_size<2:\n",
        "        continue\n",
        "      else:\n",
        "        if rnd and pop_size>sample_size:\n",
        "          sample=random.sample(names,sample_size)\n",
        "        else:\n",
        "          sample=names\n",
        "\n",
        "      #print(\"Sample: \",sample)\n",
        "\n",
        "      for ind in sample:\n",
        "        nonmatch=\"\"\n",
        "        #print(ind)\n",
        "        key=str(ind)\n",
        "        pair=random.sample(inds[key],2)\n",
        "        triples[0][cnt,:,:,:]=pair[0]\n",
        "        triples[1][cnt,:,:,:]=pair[1]\n",
        "        while len(nonmatch)==0: \n",
        "          x = str(random.sample(names,1)[0])\n",
        "          #print(x)\n",
        "          if x!=key:\n",
        "            nonmatch=x\n",
        "            #print(nonmatch)\n",
        "        triples[2][cnt,:,:,:]=random.sample(inds[nonmatch],1)[0]\n",
        "        cnt=cnt+1\n",
        "        #print(\"Iteration complete: \",cnt)\n",
        "        if cnt==batch_size:\n",
        "          break\n",
        "      if cnt==batch_size:\n",
        "        break\n",
        "  target=np.zeros((batch_size,768))\n",
        "  return triples,target\n",
        "\n",
        "\n",
        "# For use when using keras.modelsfit_generator\n",
        "def batch_gen(batch_size=20,rnd=False):\n",
        "  #print(\"IN!\")\n",
        "  while True:\n",
        "    triples,targets=GetTriples(batch_size,rnd)\n",
        "    x= (triples,targets)\n",
        "    #print(len(triples))\n",
        "    yield (triples,targets)\n",
        "\n",
        "\n",
        "#CUstom loss function for Triplets Network\n",
        "def triplet_loss(y_true,y_pred,alpha=1.0):\n",
        "  ln=y_pred.shape.as_list()[-1]\n",
        "  anchor=y_pred[:,0:int(ln/3)]\n",
        "  positive=y_pred[:,int(ln/3):int(2*ln/3)]\n",
        "  negative=y_pred[:,int(2*ln/3):ln]\n",
        "\n",
        "  p_dist=K.sqrt(K.sum(K.square(anchor-positive),axis=1))\n",
        "  n_dist=K.sqrt(K.sum(K.square(anchor-negative),axis=1))\n",
        "  loss=K.maximum(p_dist-n_dist+alpha,0.0)\n",
        "  return K.mean(loss)  \n",
        "\n",
        "\n",
        "def calcl2(X,prints):\n",
        "  l2norm=[]\n",
        "  for i in range(len(prints)):\n",
        "    l2norm.append(np.linalg.norm(X - prints[i]))\n",
        "  return l2norm\n",
        "\n",
        "def Validate(test_data,master_DB,trained_model):\n",
        "  X=test_data[0]\n",
        "  species=test_data[1]\n",
        "  distary=[]\n",
        "  indary=[]\n",
        "  support_DB=master_DB[species]\n",
        "  for individual,prints in support_DB.items():\n",
        "    prints=np.asarray(prints)\n",
        "    prints_encoded=trained_model.predict(prints)\n",
        "    dist=calcl2(X,prints_encoded)\n",
        "    ind=[individual]*len(dist)\n",
        "    distary.extend(dist)\n",
        "    indary.extend(ind)\n",
        "  order=np.argsort(distary)\n",
        "  #print(distary)\n",
        "  #print(order)\n",
        "  #print(indary)\n",
        "  #print(\"Target Individual: \",true_ind)\n",
        "  #print(distary[i],indary[i])\n",
        "  return indary[order[0]],indary[order[1]],indary[order[2]]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiR7grte6YRh",
        "colab_type": "text"
      },
      "source": [
        "### Set up Triplets Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICveywr23Jtb",
        "colab_type": "code",
        "outputId": "1aa68945-f878-4303-d43d-7a375f20173c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "#STart with VGG model used for SPecies classification (assumes loaded per previous section)\n",
        "vgg_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "vgg16 (Model)                (None, 7, 7, 512)         14714688  \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "Dense1 (Dense)               (None, 256)               6422784   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 8)                 520       \n",
            "=================================================================\n",
            "Total params: 21,179,144\n",
            "Trainable params: 21,179,144\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxGNZHAQz6Aw",
        "colab_type": "code",
        "outputId": "e6f89a1f-d43c-4236-d163-3bc72f57c63d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "#Create Triplets Model Network\n",
        "\n",
        "x=vgg_model.get_layer('Dense1').output\n",
        "x = Lambda(lambda  x: K.l2_normalize(x,axis=1))(x)\n",
        "triplet_model=Model(inputs=vgg_model.input,outputs=x)\n",
        "input_shape=[224,224,3]\n",
        "X1=Input(input_shape)\n",
        "X2=Input(input_shape)\n",
        "X3=Input(input_shape)\n",
        "encoded1 = triplet_model(X1)\n",
        "encoded2 = triplet_model(X2)\n",
        "encoded3 = triplet_model(X3)\n",
        "\n",
        "concat_vector=concatenate([encoded1,encoded2,encoded3],axis=-1,name='concat')\n",
        "model=Model(inputs=[X1,X2,X3],outputs=concat_vector)\n",
        "model.compile(loss=triplet_loss,optimizer=Adam(0.00001))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_6 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_7 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_8 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "model (Model)                   (None, 256)          21137472    input_6[0][0]                    \n",
            "                                                                 input_7[0][0]                    \n",
            "                                                                 input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concat (Concatenate)            (None, 768)          0           model[1][0]                      \n",
            "                                                                 model[2][0]                      \n",
            "                                                                 model[3][0]                      \n",
            "==================================================================================================\n",
            "Total params: 21,137,472\n",
            "Trainable params: 21,137,472\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwVwSYlM6fl_",
        "colab_type": "text"
      },
      "source": [
        "### First time Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxIEPYaBCC5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "triples,targets=GetTriples(3000,True)\n",
        "Anchor = triples[0]\n",
        "Positive = triples[1]\n",
        "Negative = triples[2]\n",
        "Y=targets\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "\n",
        "mc = ModelCheckpoint('/content/drive/My Drive/WildAI/csv/best_model.h5', monitor='val_loss', mode='min')\n",
        "\n",
        "model.fit([Anchor,Positive,Negative],y=targets, batch_size=50, epochs=120,verbose=2,validation_split=0.1,callbacks=[es,mc])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP4_jnqBUriC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "model.save_weights(\"ind-model.h5\")\n",
        "#trained_model=Model(inputs=X1,outputs=encoded1)\n",
        "#trained_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "#trained_model.load_weights(\"ind-model.h5\")\n",
        "#trained_model.load_weights(\"/content/drive/My Drive/U C Berkeley - Darragh/csv/best_model.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RA5fgtqA7G_b",
        "colab_type": "text"
      },
      "source": [
        "### EValuate /Test trained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND3Rd1vtbvY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load previously trained model\n",
        "trained_model=Model(inputs=X1,outputs=encoded1)\n",
        "trained_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "#trained_model.load_weights(\"ind-model.h5\")\n",
        "trained_model.load_weights(\"/content/drive/My Drive/WildAI/csv/best_model.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kycVWfTBZ5tb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xGJg8OeX8PA",
        "colab_type": "text"
      },
      "source": [
        "### Updated Method: using Mean Reference Evmeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Duc-si1IusGM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ALternate to KNNs - Establish a Mean vector for each Individual\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "\n",
        "\n",
        "def create_reference(input,type=0):\n",
        "  if type==0:   #Simple Mean\n",
        "    result=np.mean(input,axis=0)\n",
        "  elif type==1:  #Remove outliers\n",
        "    std=np.std(input,axis=0, dtype=np.float64)\n",
        "    mean=np.mean(input,axis=0,dtype=np.float64)\n",
        "    cutoff=std\n",
        "    ll=mean-cutoff\n",
        "    ul=mean+cutoff\n",
        "    new_mean=np.zeros(input.shape[1])\n",
        "    for j in range(input.shape[1]):\n",
        "      keep=[]\n",
        "      for i in range(input.shape[0]):\n",
        "        if input[i,j]<ll[j] or input[i,j]>ul[j]:\n",
        "          continue\n",
        "        else:\n",
        "          keep.append(input[i,j])\n",
        "      new_mean[j]=np.mean(np.array(keep))\n",
        "    result=new_mean\n",
        "  elif type==2:  #Random Isolation Forests\n",
        "    clf = IsolationForest( max_samples=10, random_state = 1, contamination= 'auto')\n",
        "    preds = clf.fit_predict(input)\n",
        "    new_mean=np.zeros(input.shape[1])\n",
        "    for j in range(input.shape[1]):\n",
        "      keep=[]\n",
        "      for i in range(input.shape[0]):\n",
        "        if preds[i]==1:\n",
        "          keep.append(input[i,j])\n",
        "      new_mean[j]=np.mean(np.array(keep))\n",
        "    result=new_mean\n",
        "\n",
        "\n",
        "\n",
        "  return result  \n",
        "\n",
        "\n",
        "\n",
        "def FindReferenceEmbeddings(DB,model):\n",
        "  Ref_Embeddings=defaultdict(defaultdict)\n",
        "  for species in DB.keys():\n",
        "    support_DB=DB[species]\n",
        "    Ref_DB=Ref_Embeddings[species]\n",
        "    X=[]\n",
        "    Y=[]\n",
        "    for individual,prints in support_DB.items():\n",
        "      if 'Unknown' in individual:\n",
        "        continue\n",
        "      else:\n",
        "        prints=np.asarray(prints)\n",
        "        prints_encoded=model.predict(prints)\n",
        "        reference_print=create_reference(prints_encoded,type=2)\n",
        "        Ref_DB[individual]=reference_print\n",
        "  return Ref_Embeddings\n",
        "\n",
        "def findnearest(Ref_Individuals,X):\n",
        "  inds=[]\n",
        "  dist=[]\n",
        "  for individual,embedding in Ref_Individuals.items():\n",
        "    inds.append(individual)\n",
        "    dist.append(np.linalg.norm(X - embedding))\n",
        "  i=np.argmin(np.asarray(dist))\n",
        "  found=inds[i]\n",
        "  return found\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psQyBpoly07B",
        "colab_type": "code",
        "outputId": "a8d4adc1-9d55-42c2-bc12-6ea6e7bd3e8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Test Results\n",
        "\n",
        "#trained_model.load_weights(\"best-model.h5\")\n",
        "Ref_Embeddings=FindReferenceEmbeddings(Ind_DB,trained_model)\n",
        "\n",
        "X_Test_encoded=trained_model.predict(X_Test)\n",
        "num=len(X_Test_encoded)\n",
        "count=defaultdict(int)\n",
        "correct_count=defaultdict(int)\n",
        "correct=0\n",
        "for i in range(num):\n",
        "  #X_encoded=trained_mode.predict(X_Test[i])  \n",
        "  x=X_Test_encoded[i]\n",
        "  species=Species_Test[i]\n",
        "  count[species]+=1\n",
        "  true=Individuals_Test[i]\n",
        "  predicted=findnearest(Ref_Embeddings[species],x.reshape(1,-1))\n",
        "  #predicted=predict(knns[species],pcas[species],x.reshape(1,-1))\n",
        "  if true==predicted:\n",
        "    correct=correct+1\n",
        "    correct_count[species]+=1\n",
        "  else:\n",
        "    print(predicted,'  ----    ',true)\n",
        "Accuracy=correct/num\n",
        "print(\"Overall Accuracy = \",Accuracy)\n",
        "for species in count.keys():\n",
        "  print(\"Accuracy for \",species,\": \",correct_count[species]/count[species] )\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_iforest.py:281: UserWarning: max_samples (10) is greater than the total number of samples (7). max_samples will be set to n_samples for estimation.\n",
            "  % (self.max_samples, n_samples))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_iforest.py:281: UserWarning: max_samples (10) is greater than the total number of samples (9). max_samples will be set to n_samples for estimation.\n",
            "  % (self.max_samples, n_samples))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_iforest.py:281: UserWarning: max_samples (10) is greater than the total number of samples (6). max_samples will be set to n_samples for estimation.\n",
            "  % (self.max_samples, n_samples))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_iforest.py:281: UserWarning: max_samples (10) is greater than the total number of samples (8). max_samples will be set to n_samples for estimation.\n",
            "  % (self.max_samples, n_samples))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_iforest.py:281: UserWarning: max_samples (10) is greater than the total number of samples (8). max_samples will be set to n_samples for estimation.\n",
            "  % (self.max_samples, n_samples))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_iforest.py:281: UserWarning: max_samples (10) is greater than the total number of samples (9). max_samples will be set to n_samples for estimation.\n",
            "  % (self.max_samples, n_samples))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_iforest.py:281: UserWarning: max_samples (10) is greater than the total number of samples (5). max_samples will be set to n_samples for estimation.\n",
            "  % (self.max_samples, n_samples))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Amur Tiger-682   ----     Amur Tiger-261\n",
            "Amur Tiger-440   ----     Amur Tiger-279\n",
            "Amur Tiger-682   ----     Amur Tiger-440\n",
            "Amur Tiger-237   ----     Amur Tiger-682\n",
            "Amur Tiger-261   ----     Amur Tiger-1020\n",
            "Bengal Tiger-India   ----     Bengal Tiger-Fenimore\n",
            "Bengal Tiger-Aria   ----     Bengal Tiger-India\n",
            "Bengal Tiger-Rajaji   ----     Bengal Tiger-Lucky\n",
            "Cheetah-Alvin   ----     Cheetah-Aiko\n",
            "Cheetah-Kiki   ----     Cheetah-Chiquita\n",
            "Cheetah-Alvin   ----     Cheetah-Pano\n",
            "Leopard-Timbila   ----     Leopard-Keanu\n",
            "Leopard-Timbila   ----     Leopard-Keanu\n",
            "Leopard-Keanu   ----     Leopard-Shakira\n",
            "Leopard-Lewa   ----     Leopard-Shakira\n",
            "Leopard-Timbila   ----     Leopard-Shakira\n",
            "Leopard-Lewa   ----     Leopard-Mick\n",
            "Leopard-Lewa   ----     Leopard-Timbila\n",
            "Leopard-Keanu   ----     Leopard-Wahoo\n",
            "Lowland Tapir-Chuvisco M   ----     Lowland Tapir-Chuva F\n",
            "Lowland Tapir-Chuva F   ----     Lowland Tapir-Feminha F\n",
            "Lowland Tapir-Sorocaba 5   ----     Lowland Tapir-Sorocaba\n",
            "Lowland Tapir-Sorocaba 5   ----     Lowland Tapir-Sorocaba 2\n",
            "Lowland Tapir-Sorocaba   ----     Lowland Tapir-Sorocaba 5\n",
            "Puma-M-Skit   ----     Puma-F-Archback\n",
            "Puma-M-Oldex   ----     Puma-F-Spots\n",
            "Puma-M-Taz   ----     Puma-M-Darby\n",
            "Puma-F-Lip   ----     Puma-M-Juvboy\n",
            "Puma-M-Skit   ----     Puma-M-Pops\n",
            "Puma-M-Skit   ----     Puma-M-Pops\n",
            "Puma-M-Oldex   ----     Puma-M-Skit\n",
            "Puma-F-Majanna   ----     Puma-M-Taz\n",
            "Black Rhino-Kal Db M2   ----     Black Rhino-Kuz Db Col\n",
            "Black Rhino-Kuz Db Col   ----     Black Rhino-Kuz Db Hec\n",
            "Overall Accuracy =  0.746268656716418\n",
            "Accuracy for  Amur Tiger :  0.5454545454545454\n",
            "Accuracy for  Bengal Tiger :  0.8125\n",
            "Accuracy for  Cheetah :  0.875\n",
            "Accuracy for  Leopard :  0.42857142857142855\n",
            "Accuracy for  Lowland Tapir :  0.5833333333333334\n",
            "Accuracy for  Puma :  0.7037037037037037\n",
            "Accuracy for  White Rhino :  1.0\n",
            "Accuracy for  Black Rhino :  0.875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wMrtEJxXoUj",
        "colab_type": "text"
      },
      "source": [
        "### Original method - Using K Nearest Neighbors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2kpi9gbJAda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use K-Newares Neighbors to evaluate training/ test results\n",
        "\n",
        "def FindKNN(X,Y,X_Test,Y_Test):\n",
        "  k_range=range(1,20)\n",
        "  scores={}\n",
        "  scores_list=[]\n",
        "  for k in k_range:\n",
        "    knn=KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X,Y)\n",
        "    Y_Pred=knn.predict(X_Test)\n",
        "    acc=metrics.accuracy_score(Y_Test,Y_Pred)\n",
        "    scores[k]=acc\n",
        "    scores_list.append(acc)\n",
        "  #print(scores)\n",
        "  return np.argmax(scores_list)\n",
        "\n",
        "\n",
        "def FitKNNs(DB,model):\n",
        "  knns={}\n",
        "  #tsnes={}\n",
        "  for species in DB.keys():\n",
        "    support_DB=DB[species]\n",
        "    X=[]\n",
        "    Y=[]\n",
        "    for individual,prints in support_DB.items():\n",
        "      if 'Unknown' in individual:\n",
        "        continue\n",
        "      else:\n",
        "        prints=np.asarray(prints)\n",
        "        prints_encoded=model.predict(prints)\n",
        "        ind=[individual]*len(prints_encoded)\n",
        "        X.extend(prints_encoded)\n",
        "        Y.extend(ind)\n",
        "    #df=pd.DataFrame(X)\n",
        "    #tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
        "    #tsne_results = tsne.fit_transform(df)\n",
        "    #XT=pd.DataFrame()\n",
        "    #YT=pd.DataFrame(Y)\n",
        "    #XT['tsne-2d-one'] = tsne_results[:,0]\n",
        "    #XT['tsne-2d-two'] = tsne_results[:,1]\n",
        "    X_Train, X_Val, Y_Train, Y_Val = train_test_split(X, Y, test_size=0.20, random_state=42)\n",
        "    #X_Train, X_Val, Y_Train, Y_Val = train_test_split(XT, YT, test_size=0.20, random_state=42)\n",
        "    k=FindKNN(X_Train,Y_Train,X_Val,Y_Val)+1\n",
        "    #print(species,\" : \",k)\n",
        "    #k=3\n",
        "    knn=KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_Train,Y_Train)\n",
        "    knns[species]=knn\n",
        "    #tsnes[species]=tsne\n",
        "  #return knns,tsnes\n",
        "  return knns\n",
        "\n",
        "def predict(knn,x):\n",
        "  #dfx=pd.DataFrame(x)\n",
        "  #tsne_results = tsne.transform(dfx)\n",
        "  #XT=pd.DataFrame()\n",
        "  #XT['tsne-2d-one'] = tsne_results[:,0]\n",
        "  #XT['tsne-2d-two'] = tsne_results[:,1]\n",
        "  predicted=knn.predict(x)\n",
        "  return(predicted)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-EY6WqtFvQy",
        "colab_type": "code",
        "outputId": "fbad7768-04d3-42da-9f61-c8f83dabc099",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "#Test Results\n",
        "\n",
        "#trained_model.load_weights(\"best-model.h5\")\n",
        "knns=FitKNNs(Ind_DB,trained_model)\n",
        "\n",
        "\n",
        "X_Test_encoded=trained_model.predict(X_Test)\n",
        "num=len(X_Test_encoded)\n",
        "count=defaultdict(int)\n",
        "correct_count=defaultdict(int)\n",
        "correct=0\n",
        "for i in range(num):\n",
        "  #X_encoded=trained_mode.predict(X_Test[i])  \n",
        "  x=X_Test_encoded[i]\n",
        "  species=Species_Test[i]\n",
        "  count[species]+=1\n",
        "  true=Individuals_Test[i]\n",
        "  predicted=predict(knns[species],x.reshape(1,-1))\n",
        "  #predicted=predict(knns[species],pcas[species],x.reshape(1,-1))\n",
        "  if true==predicted[0]:\n",
        "    correct=correct+1\n",
        "    correct_count[species]+=1\n",
        "  else:\n",
        "    print(predicted[0],'  ----    ',true)\n",
        "Accuracy=correct/num\n",
        "print(\"Overall Accuracy = \",Accuracy)\n",
        "for species in count.keys():\n",
        "  print(\"Accuracy for \",species,\": \",correct_count[species]/count[species] )\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Amur Tiger-682   ----     Amur Tiger-261\n",
            "Amur Tiger-440   ----     Amur Tiger-279\n",
            "Amur Tiger-682   ----     Amur Tiger-440\n",
            "Amur Tiger-237   ----     Amur Tiger-682\n",
            "Amur Tiger-237   ----     Amur Tiger-682\n",
            "Amur Tiger-261   ----     Amur Tiger-1020\n",
            "Bengal Tiger-India   ----     Bengal Tiger-Fenimore\n",
            "Bengal Tiger-Aria   ----     Bengal Tiger-India\n",
            "Bengal Tiger-Rajaji   ----     Bengal Tiger-Lucky\n",
            "Bengal Tiger-India   ----     Bengal Tiger-Moki\n",
            "Cheetah-Kiki   ----     Cheetah-Chiquita\n",
            "Cheetah-Tearmark   ----     Cheetah-Chiquita\n",
            "Cheetah-Alvin   ----     Cheetah-Pano\n",
            "Leopard-Timbila   ----     Leopard-Keanu\n",
            "Leopard-Timbila   ----     Leopard-Keanu\n",
            "Leopard-Keanu   ----     Leopard-Shakira\n",
            "Leopard-Lewa   ----     Leopard-Shakira\n",
            "Leopard-Timbila   ----     Leopard-Shakira\n",
            "Leopard-Lewa   ----     Leopard-Mick\n",
            "Leopard-Lewa   ----     Leopard-Timbila\n",
            "Leopard-Keanu   ----     Leopard-Wahoo\n",
            "Lowland Tapir-Chuvisco M   ----     Lowland Tapir-Chuva F\n",
            "Lowland Tapir-Chuva F   ----     Lowland Tapir-Feminha F\n",
            "Lowland Tapir-Sorocaba 5   ----     Lowland Tapir-Sorocaba 2\n",
            "Lowland Tapir-Sorocaba   ----     Lowland Tapir-Sorocaba 5\n",
            "Puma-M-Pops   ----     Puma-F-Archback\n",
            "Puma-M-Pops   ----     Puma-F-Spots\n",
            "Puma-M-Taz   ----     Puma-M-Darby\n",
            "Puma-M-Pops   ----     Puma-M-Skit\n",
            "Puma-F-Majanna   ----     Puma-M-Taz\n",
            "Black Rhino-Kal Db M2   ----     Black Rhino-Kuz Db Col\n",
            "Black Rhino-Kuz Db Hel   ----     Black Rhino-Kuz Db Hec\n",
            "Overall Accuracy =  0.7611940298507462\n",
            "Accuracy for  Amur Tiger :  0.45454545454545453\n",
            "Accuracy for  Bengal Tiger :  0.75\n",
            "Accuracy for  Cheetah :  0.875\n",
            "Accuracy for  Leopard :  0.42857142857142855\n",
            "Accuracy for  Lowland Tapir :  0.6666666666666666\n",
            "Accuracy for  Puma :  0.8148148148148148\n",
            "Accuracy for  White Rhino :  1.0\n",
            "Accuracy for  Black Rhino :  0.875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoSlq598V7jt",
        "colab_type": "code",
        "outputId": "9142b311-58b0-4829-a34e-513f4d16a650",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "source": [
        "#ALternate prediction (WORK IN PROGRESS)\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "def norm_predict(x):\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-89-e48d350d99cf>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sbZe3zUM7lm",
        "colab_type": "text"
      },
      "source": [
        "## Visualization of Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzY-gwBiM-QS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtfgxoU40ozV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y=le.transform(Species)\n",
        "X_encoded=trained_model.predict(X)\n",
        "le.classes_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wyiC30o6OL2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1=pd.DataFrame(X_encoded)\n",
        "df1['y']=Y\n",
        "df1['Names']=Individuals \n",
        "df_ATiger=df1[df1.y==0]\n",
        "\n",
        "df_BTiger=df1[df1.y==1]\n",
        "\n",
        "df_BRhino=df1[df1.y==2]\n",
        "\n",
        "df_Cheetah=df1[df1.y==3]\n",
        "\n",
        "df_Leopard=df1[df1.y==4]\n",
        "\n",
        "df_LTapir=df1[df1.y==5]\n",
        "\n",
        "df_Puma=df1[df1.y==6]\n",
        "\n",
        "df_WRhino=df1[df1.y==7]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9j290kM6duU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plotprints(df):\n",
        "  dfx=df.drop(['y','Names'],axis=1)\n",
        "  tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
        "  tsne_results = tsne.fit_transform(dfx)\n",
        "  df['tsne-2d-one'] = tsne_results[:,0]\n",
        "  df['tsne-2d-two'] = tsne_results[:,1]\n",
        "  plt.figure(figsize=(16,10))\n",
        "  num=df['Names'].nunique()\n",
        "\n",
        "  sns.scatterplot(x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n",
        "                  hue=\"Names\",\n",
        "                  palette=sns.color_palette(\"hls\", num),\n",
        "                  data=df,\n",
        "                  legend=\"full\",\n",
        "                  alpha=0.6)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDd0RrSeIHkv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Plot projected 2D clusters for each species\n",
        "plotprints(df_WRhino)\n",
        "plotprints(df_BRhino)\n",
        "plotprints(df_ATiger)\n",
        "plotprints(df_BTiger)\n",
        "plotprints(df_Puma)\n",
        "plotprints(df_Cheetah)\n",
        "plotprints(df_Leopard)\n",
        "plotprints(df_LTapir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PM6fb0vDbhAa",
        "colab_type": "text"
      },
      "source": [
        "# MobileNetV2 - UNDER CONSTRUCTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KH_RONF7hgM",
        "colab_type": "text"
      },
      "source": [
        "Placeholder Section to reproduce work done above (with VGG16) using MobileNet V2 . \n",
        "Note: INCOMPLETE: Needs to be completed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfYuX5Cncmm3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_shape=(224,224,3)\n",
        "mnet=MobileNetV2(weights='imagenet',include_top=False,input_shape=input_shape)\n",
        "mnet.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScDUSDYnKDxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load Pre-Processed Images\n",
        "  csvpath='/content/drive/My Drive/U C Berkeley - Darragh/csv'\n",
        "  X,Species,Individuals, Ind_DB,X_Test,Species_Test,Individuals_Test=LoadData(\"Train-Images-Mobile-224.csv\",\"Train-Labels-Mobile-224.txt\",\"Test-Images-Mobile-224.csv\",\"Test-Labels-Mobile-224.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvdbsEjQfru1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "le = LabelEncoder()\n",
        "le.fit(Species)\n",
        "Y=le.transform(Species)\n",
        "Y_Test=le.transform(Species_Test)\n",
        "Y1=to_categorical(np.array(Y))\n",
        "Y_Test1=to_categorical(np.array(Y_Test))\n",
        "print(Y1.shape)\n",
        "print(le.classes_)\n",
        "#For Species Classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_Train, X_Val, Y_Train, Y_Val = train_test_split(X, Y1, test_size=0.10, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ej2rXtYhDbu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "mnet_model=Sequential()\n",
        "mnet_model.add(mnet)\n",
        "mnet_model.add(Flatten())\n",
        "mnet_model.add(Dense(128, activation='relu',name=\"Dense1\"))\n",
        "mnet_model.add(Dense(64, activation='relu',name=\"Dense2\"))\n",
        "mnet_model.add(Dropout(0.8))\n",
        "mnet_model.add(Dense(8))\n",
        "\n",
        "mnet_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G22beqV1gef2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "mnet_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.00001),\n",
        "              loss=loss_fn,\n",
        "              metrics=['accuracy'])\n",
        "mnet_model.fit(X_Train,Y_Train,validation_data=(X_Val,Y_Val),epochs=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVbHX4EOiCIT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnet_model.evaluate(X_Test,  Y_Test1, verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMGEpl9ZAstg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QtyjVIhltb0",
        "colab_type": "text"
      },
      "source": [
        "### Identification Task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2Ma_mKBmMfb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnet_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kwcbF09mKYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "triplet_model=Model(inputs=mnet_model.input,outputs=mnet_model.get_layer('Dense1').output)\n",
        "triplet_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2x90nuj1mfzT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_shape=[224,224,3]\n",
        "X1=Input(input_shape)\n",
        "X2=Input(input_shape)\n",
        "X3=Input(input_shape)\n",
        "encoded1 = triplet_model(X1)\n",
        "encoded2 = triplet_model(X2)\n",
        "encoded3 = triplet_model(X3)\n",
        "\n",
        "concat_vector=concatenate([encoded1,encoded2,encoded3],axis=-1,name='concat')\n",
        "model=Model(inputs=[X1,X2,X3],outputs=concat_vector)\n",
        "model.compile(loss=triplet_loss,optimizer=Adam(0.00001))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCGE5istmwka",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "triples,targets=GetTriples(1500,True)\n",
        "Anchor = triples[0]\n",
        "Positive = triples[1]\n",
        "Negative = triples[2]\n",
        "Y=targets\n",
        "\n",
        "model.fit([Anchor,Positive,Negative],y=targets, batch_size=40, epochs=60,verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJ00kHl8sTe2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights(\"ind-model-mnet.h5\")\n",
        "trained_model=Model(inputs=X1,outputs=encoded1)\n",
        "trained_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "trained_model.load_weights(\"ind-model-mnet.h5\")\n",
        "\n",
        "num=len(X_Test)\n",
        "correct=0\n",
        "X_encoded=trained_model.predict(X_Test)\n",
        "\n",
        "for i in range(num):\n",
        "  #img=x = np.expand_dims(X_Test[i], axis=0)\n",
        "  #X_encoded=trained_model.predict(X_Test[i])  \n",
        "  test_data=[X_encoded[i],Species_Test[i]]\n",
        "  true=Individuals_Test[i]\n",
        "  predicted,silver,bronze=Validate(test_data,Ind_DB,trained_model)\n",
        "  print(true,predicted,silver,bronze)\n",
        "  if true==predicted:\n",
        "    correct=correct+1\n",
        "Accuracy=correct/num\n",
        "print(\"Accuracy = \",Accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}